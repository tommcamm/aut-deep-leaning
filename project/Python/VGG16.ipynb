{
  "cells": [
    {
      "metadata": {
        "id": "d9affeac745701b2"
      },
      "cell_type": "markdown",
      "source": [
        "# Pure CNN using VGG16\n",
        "\n",
        "This notebook is a simple CNN using VGG16 architecture."
      ],
      "id": "d9affeac745701b2"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-13T00:23:54.915543Z",
          "start_time": "2024-06-13T00:23:52.478428Z"
        },
        "id": "7a377d493afc7659"
      },
      "cell_type": "code",
      "source": [
        "# Libraries import\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.transforms import functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n"
      ],
      "id": "7a377d493afc7659",
      "outputs": [],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "c286b1ed36150f7d",
        "outputId": "be4f930d-fbc2-4a79-e480-d88c3d9d9d95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "execution_count": 7,
      "source": [
        "# Colab setup\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    !mkdir -p ./Datasets\n",
        "    # copy dataset to local dir\n",
        "    !cp /content/drive/MyDrive/Datasets/YOLO_UODD_dataset.zip ./Datasets/YOLO_UODD_dataset.zip\n",
        "    !unzip -q ./Datasets/YOLO_UODD_dataset.zip -d ./Datasets"
      ],
      "id": "c286b1ed36150f7d"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-13T00:34:48.343231Z",
          "start_time": "2024-06-13T00:34:47.333322Z"
        },
        "id": "ebf9b3c19c42bcce"
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import CocoDetection\n",
        "\n",
        "class UODDDataset(Dataset):\n",
        "    def __init__(self, root, annotation_file, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        with open(annotation_file) as f:\n",
        "            self.coco = json.load(f)\n",
        "        self.ids = list(sorted(self.coco['images'], key=lambda x: x['id']))\n",
        "        # Filter images with annotations\n",
        "        self.ids = [img for img in self.ids if self.has_annotations(img['id'])]\n",
        "\n",
        "    def has_annotations(self, img_id):\n",
        "        ann_ids = [ann for ann in self.coco['annotations'] if ann['image_id'] == img_id]\n",
        "        return len(ann_ids) > 0\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.ids[idx]['id']\n",
        "        img_info = self.ids[idx]\n",
        "        img_path = os.path.join(self.root, img_info['file_name'])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        ann_ids = [ann for ann in self.coco['annotations'] if ann['image_id'] == img_id]\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in ann_ids:\n",
        "            xmin, ymin, w, h = ann['bbox']\n",
        "            boxes.append([xmin, ymin, xmin + w, ymin + h])\n",
        "            labels.append(ann['category_id'])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "\n",
        "def get_transform():\n",
        "    transforms = []\n",
        "    transforms.append(F.to_tensor)\n",
        "    return torchvision.transforms.Compose(transforms)\n",
        "\n",
        "# Cell 5: Load data\n",
        "train_data = UODDDataset('./Datasets/YOLO_UODD/images/train', './Datasets/YOLO_UODD/annotations/coco/instances_train.json', transforms=get_transform())\n",
        "val_data = UODDDataset('./Datasets/YOLO_UODD/images/val', './Datasets/YOLO_UODD/annotations/coco/instances_val.json', transforms=get_transform())\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "val_loader = DataLoader(val_data, batch_size=8, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
      ],
      "id": "ebf9b3c19c42bcce",
      "outputs": [],
      "execution_count": 14
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-13T00:23:57.707755Z",
          "start_time": "2024-06-13T00:23:57.704054Z"
        },
        "id": "a05e781f4f2a3202",
        "outputId": "b9b5189b-0d69-4b19-874f-0aabc68cd815",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "print(f\"Train dataset: {len(train_data)}\")\n",
        "print(f\"Validation dataset: {len(val_data)}\")"
      ],
      "id": "a05e781f4f2a3202",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset: 2558\n",
            "Validation dataset: 128\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-13T00:24:03.163669Z",
          "start_time": "2024-06-13T00:24:00.609389Z"
        },
        "id": "5c217fa65174c8b5"
      },
      "cell_type": "code",
      "source": [
        "from torchvision.models import VGG16_Weights\n",
        "\n",
        "backbone = torchvision.models.vgg16(weights=VGG16_Weights.DEFAULT).features\n",
        "backbone.out_channels = 512\n",
        "\n",
        "# Define the RPN anchor generator\n",
        "rpn_anchor_generator = AnchorGenerator(\n",
        "    sizes=((32, 64, 128, 256, 512),),\n",
        "    aspect_ratios=((0.5, 1.0, 2.0),) * 1\n",
        ")\n",
        "\n",
        "# Define the RoI pooler\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)\n",
        "\n",
        "# Create the Faster R-CNN model\n",
        "model = FasterRCNN(backbone, num_classes=5,\n",
        "                   rpn_anchor_generator=rpn_anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)"
      ],
      "id": "5c217fa65174c8b5",
      "outputs": [],
      "execution_count": 16
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-13T00:29:03.279938Z",
          "start_time": "2024-06-13T00:24:07.003522Z"
        },
        "id": "119cb82fe2eee61a",
        "outputId": "bacd0523-ec89-44d4-a431-d2a69ce75efb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Training setup\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "# Training function with progress bar and loss printing\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    num_batches = len(data_loader)\n",
        "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
        "\n",
        "    for images, targets in progress_bar:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        running_loss += losses.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        progress_bar.set_postfix(loss=losses.item())\n",
        "\n",
        "    epoch_loss = running_loss / num_batches\n",
        "    print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Training loop with tqdm progress bar\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
        "    lr_scheduler.step()\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'fasterrcnn_vgg16_uodd.pth')\n",
        "\n",
        "print(\"Training complete. Model saved as 'fasterrcnn_vgg16_uodd.pth'.\")"
      ],
      "id": "119cb82fe2eee61a",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 320/320 [02:08<00:00,  2.50batch/s, loss=0.293]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 0.4504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 320/320 [02:05<00:00,  2.55batch/s, loss=0.301]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 0.3366\n",
            "Training complete. Model saved as 'fasterrcnn_vgg16_uodd.pth'.\n"
          ]
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  !cp ./fasterrcnn_vgg16_uodd.pth /content/drive/MyDrive/Models/fasterrcnn_vgg16_uodd.pth"
      ],
      "metadata": {
        "id": "Mx7j2vi0EMKs"
      },
      "id": "Mx7j2vi0EMKs",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "\n",
        "val_root = './Datasets/YOLO_UODD/images/val'\n",
        "val_annFile = './Datasets/YOLO_UODD/annotations/coco/instances_val.json'\n",
        "val_dataset = UODDDataset(val_root, val_annFile, transforms=get_transform())\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    coco = COCO(val_annFile)\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            images = list(img.to(device) for img in images)\n",
        "            outputs = model(images)\n",
        "\n",
        "            for target, output in zip(targets, outputs):\n",
        "                image_id = target[\"image_id\"].item()\n",
        "                boxes = output[\"boxes\"].cpu().numpy()\n",
        "                scores = output[\"scores\"].cpu().numpy()\n",
        "                labels = output[\"labels\"].cpu().numpy()\n",
        "\n",
        "                for box, score, label in zip(boxes, scores, labels):\n",
        "                    result = {\n",
        "                        \"image_id\": image_id,\n",
        "                        \"category_id\": label,\n",
        "                        \"bbox\": box.tolist(),\n",
        "                        \"score\": score\n",
        "                    }\n",
        "                    results.append(result)\n",
        "\n",
        "    # Save results in COCO format\n",
        "    with open(\"results.json\", \"w\") as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "    coco_dt = coco.loadRes(\"results.json\")\n",
        "    coco_eval = COCOeval(coco, coco_dt, iouType='bbox')\n",
        "\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    # Extract metrics\n",
        "    precision = coco_eval.stats[0]  # Precision @ IoU=0.50\n",
        "    recall = coco_eval.stats[1]     # Recall @ IoU=0.50\n",
        "    mAP_50 = coco_eval.stats[1]     # mAP @ IoU=0.50\n",
        "    mAP_50_95 = coco_eval.stats[0]  # mAP @ IoU=0.50:0.95\n",
        "\n",
        "    return precision, recall, mAP_50, mAP_50_95\n",
        "\n",
        "# Evaluate the model\n",
        "precision, recall, mAP_50, mAP_50_95 = evaluate(model, val_loader, device)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"mAP@50: {mAP_50:.4f}\")\n",
        "print(f\"mAP@50-95: {mAP_50_95:.4f}\")"
      ],
      "metadata": {
        "id": "QKvqNyzoDdKJ",
        "outputId": "a67569a4-1759-4b46-b749-0b08af5e24fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "id": "QKvqNyzoDdKJ",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:   0%|          | 0/64 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'image_id'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e21981b1e786>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmAP_50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmAP_50_95\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Precision: {precision:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-e21981b1e786>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mimage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scores\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'image_id'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}