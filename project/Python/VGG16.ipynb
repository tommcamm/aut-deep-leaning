{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pure CNN using VGG16\n",
    "\n",
    "This notebook is a simple CNN using VGG16 architecture."
   ],
   "id": "d9affeac745701b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T00:23:54.915543Z",
     "start_time": "2024-06-13T00:23:52.478428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Libraries import\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n"
   ],
   "id": "7a377d493afc7659",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Colab setup\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # copy dataset to local dir\n",
    "    !cp /content/drive/MyDrive/Datasets/YOLO_UODD_dataset.zip ./datasets/YOLO_UODD_dataset.zip\n",
    "    !unzip -q ./datasets/YOLO_UODD_dataset.zip -d ./datasets"
   ],
   "id": "c286b1ed36150f7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T00:34:48.343231Z",
     "start_time": "2024-06-13T00:34:47.333322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import CocoDetection\n",
    "\n",
    "class UODDDataset(Dataset):\n",
    "    def __init__(self, root, annotation_file, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        with open(annotation_file) as f:\n",
    "            self.coco = json.load(f)\n",
    "        self.ids = list(sorted(self.coco['images'], key=lambda x: x['id']))\n",
    "        # Filter images with annotations\n",
    "        self.ids = [img for img in self.ids if self.has_annotations(img['id'])]\n",
    "\n",
    "    def has_annotations(self, img_id):\n",
    "        ann_ids = [ann for ann in self.coco['annotations'] if ann['image_id'] == img_id]\n",
    "        return len(ann_ids) > 0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]['id']\n",
    "        img_info = self.ids[idx]\n",
    "        img_path = os.path.join(self.root, img_info['file_name'])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        ann_ids = [ann for ann in self.coco['annotations'] if ann['image_id'] == img_id]\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in ann_ids:\n",
    "            xmin, ymin, w, h = ann['bbox']\n",
    "            boxes.append([xmin, ymin, xmin + w, ymin + h])\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "\n",
    "def get_transform():\n",
    "    transforms = []\n",
    "    transforms.append(F.to_tensor)\n",
    "    return torchvision.transforms.Compose(transforms)\n",
    "\n",
    "# Cell 5: Load data\n",
    "train_data = UODDDataset('./Datasets/Underwater-object-detection-dataset/imgs/train', './Datasets/Underwater-object-detection-dataset/annotations/coco/instances_train.json', transforms=get_transform())\n",
    "val_data = UODDDataset('./Datasets/Underwater-object-detection-dataset/imgs/val', './Datasets/Underwater-object-detection-dataset/annotations/coco/instances_val.json', transforms=get_transform())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_data, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
   ],
   "id": "ebf9b3c19c42bcce",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T00:23:57.707755Z",
     "start_time": "2024-06-13T00:23:57.704054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Train dataset: {len(train_data)}\")\n",
    "print(f\"Validation dataset: {len(val_data)}\")"
   ],
   "id": "a05e781f4f2a3202",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 2560\n",
      "Validation dataset: 128\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T00:24:03.163669Z",
     "start_time": "2024-06-13T00:24:00.609389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "backbone = torchvision.models.vgg16(weights=VGG16_Weights.DEFAULT).features\n",
    "backbone.out_channels = 512\n",
    "\n",
    "# Define the RPN anchor generator\n",
    "rpn_anchor_generator = AnchorGenerator(\n",
    "    sizes=((32, 64, 128, 256, 512),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),) * 1\n",
    ")\n",
    "\n",
    "# Define the RoI pooler\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)\n",
    "\n",
    "# Create the Faster R-CNN model\n",
    "model = FasterRCNN(backbone, num_classes=5,\n",
    "                   rpn_anchor_generator=rpn_anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ],
   "id": "5c217fa65174c8b5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T00:29:03.279938Z",
     "start_time": "2024-06-13T00:24:07.003522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Training setup\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training function with progress bar and loss printing\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = len(data_loader)\n",
    "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
    "\n",
    "    for images, targets in progress_bar:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        running_loss += losses.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_postfix(loss=losses.item())\n",
    "\n",
    "    epoch_loss = running_loss / num_batches\n",
    "    print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Training loop with tqdm progress bar\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'fasterrcnn_vgg16_uodd.pth')\n",
    "\n",
    "print(\"Training complete. Model saved as 'fasterrcnn_vgg16_uodd.pth'.\")"
   ],
   "id": "119cb82fe2eee61a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  44%|████▍     | 561/1280 [04:55<06:18,  1.90batch/s, loss=0.253] \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected target boxes to be a tensor of shape [N, 4], got torch.Size([0]).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 37\u001B[0m\n\u001B[0;32m     35\u001B[0m num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m---> 37\u001B[0m     train_one_epoch(model, optimizer, train_loader, device, epoch)\n\u001B[0;32m     38\u001B[0m     lr_scheduler\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# Save the model\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[6], line 21\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[1;34m(model, optimizer, data_loader, device, epoch)\u001B[0m\n\u001B[0;32m     19\u001B[0m images \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(img\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m images)\n\u001B[0;32m     20\u001B[0m targets \u001B[38;5;241m=\u001B[39m [{k: v\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m t\u001B[38;5;241m.\u001B[39mitems()} \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m targets]\n\u001B[1;32m---> 21\u001B[0m loss_dict \u001B[38;5;241m=\u001B[39m model(images, targets)\n\u001B[0;32m     22\u001B[0m losses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(loss \u001B[38;5;28;01mfor\u001B[39;00m loss \u001B[38;5;129;01min\u001B[39;00m loss_dict\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[0;32m     23\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m losses\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AUT-DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AUT-DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AUT-DL\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:67\u001B[0m, in \u001B[0;36mGeneralizedRCNN.forward\u001B[1;34m(self, images, targets)\u001B[0m\n\u001B[0;32m     65\u001B[0m boxes \u001B[38;5;241m=\u001B[39m target[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mboxes\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(boxes, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m---> 67\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_assert(\n\u001B[0;32m     68\u001B[0m         \u001B[38;5;28mlen\u001B[39m(boxes\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m boxes\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m4\u001B[39m,\n\u001B[0;32m     69\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected target boxes to be a tensor of shape [N, 4], got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mboxes\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     70\u001B[0m     )\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_assert(\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected target boxes to be of type Tensor, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(boxes)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AUT-DL\\Lib\\site-packages\\torch\\__init__.py:1499\u001B[0m, in \u001B[0;36m_assert\u001B[1;34m(condition, message)\u001B[0m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(condition) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mTensor \u001B[38;5;129;01mand\u001B[39;00m has_torch_function((condition,)):\n\u001B[0;32m   1498\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001B[1;32m-> 1499\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m condition, message\n",
      "\u001B[1;31mAssertionError\u001B[0m: Expected target boxes to be a tensor of shape [N, 4], got torch.Size([0])."
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
