@inproceedings{carionEndtoEndObjectDetection2020,
  title = {End-to-{{End Object Detection}} with {{Transformers}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  pages = {213--229},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-58452-8_13},
  abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  isbn = {978-3-030-58452-8},
  langid = {english},
  file = {C:\Users\tommc\Zotero\storage\5ZSWLH76\Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf}
}

@online{dumoulinGuideConvolutionArithmetic2018,
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  author = {Dumoulin, Vincent and Visin, Francesco},
  date = {2018-01-11},
  eprint = {1603.07285},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1603.07285},
  url = {http://arxiv.org/abs/1603.07285},
  urldate = {2024-06-10},
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\tommc\\Zotero\\storage\\8HFLCGHG\\Dumoulin and Visin - 2018 - A guide to convolution arithmetic for deep learnin.pdf;C\:\\Users\\tommc\\Zotero\\storage\\BPYFRBCT\\1603.html}
}

@article{fayazUnderwaterObjectDetection2022,
  title = {Underwater Object Detection: Architectures and Algorithms – a Comprehensive Review},
  shorttitle = {Underwater Object Detection},
  author = {Fayaz, Sheezan and Parah, Shabir A. and Qureshi, G. J.},
  date = {2022-06-01},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {81},
  number = {15},
  pages = {20871--20916},
  issn = {1573-7721},
  doi = {10.1007/s11042-022-12502-1},
  url = {https://doi.org/10.1007/s11042-022-12502-1},
  urldate = {2024-03-26},
  abstract = {Underwater object detection is an essential step in image processing and it plays a vital role in several applications such as the repair and maintenance of sub-aquatic structures and marine sciences. Many computer vision-based solutions have been proposed but an optimal solution for underwater object detection and species classification does not exist. This is mainly because of the challenges presented by the underwater environment which mainly include light scattering and light absorption. The advent of deep learning has enabled researchers to solve various problems like protection of the subaquatic ecological environment, emergency rescue, reducing chances of underwater disaster and its prevention, underwater target detection, spooring, and recognition. However, the advantages and shortcomings of these deep learning algorithms are still unclear. Thus, to give a clearer view of the underwater object detection algorithms and their pros and cons, we proffer a state-of-the-art review of different computer vision-based approaches that have been developed as yet. Besides, a comparison of various state-of-the-art schemes is made based on various objective indices and future research directions in the field of underwater object detection have also been proffered.},
  langid = {english},
  keywords = {Deep-learning,Light absorption,Light scattering,Object detection},
  file = {C:\Users\tommc\Zotero\storage\TJNNPMY2\Fayaz et al. - 2022 - Underwater object detection architectures and alg.pdf}
}

@article{hanSurveyVisionTransformer2023,
  title = {A {{Survey}} on {{Vision Transformer}}},
  author = {Han, Kai and Wang, Yunhe and Chen, Hanting and Chen, Xinghao and Guo, Jianyuan and Liu, Zhenhua and Tang, Yehui and Xiao, An and Xu, Chunjing and Xu, Yixing and Yang, Zhaohui and Zhang, Yiman and Tao, Dacheng},
  date = {2023-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {1},
  pages = {87--110},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2022.3152247},
  url = {https://ieeexplore.ieee.org/abstract/document/9716741?casa_token=cvKglGOvwU8AAAAA:PMBISUAvS1ek3fnhlEuMfJaCkdEwiG1odiypLJc0lllLa9nCK7HytLh8VjQRzeDMH8nlPExRZDQ},
  urldate = {2024-04-02},
  abstract = {Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {Computational modeling,Computer vision,Encoding,high-level vision,low-level vision,Object detection,self-attention,Task analysis,transformer,Transformers,video,Visualization},
  file = {C\:\\Users\\tommc\\Zotero\\storage\\LNQRBYUI\\Han et al. - 2023 - A Survey on Vision Transformer.pdf;C\:\\Users\\tommc\\Zotero\\storage\\GED3ILGS\\9716741.html}
}

@article{hanUnderwaterImageProcessing2020,
  title = {Underwater {{Image Processing}} and {{Object Detection Based}} on {{Deep CNN Method}}},
  author = {Han, Fenglei and Yao, Jingzheng and Zhu, Haitao and Wang, Chunhui},
  date = {2020-05-22},
  journaltitle = {Journal of Sensors},
  volume = {2020},
  pages = {e6707328},
  publisher = {Hindawi},
  issn = {1687-725X},
  doi = {10.1155/2020/6707328},
  url = {https://www.hindawi.com/journals/js/2020/6707328/},
  urldate = {2024-04-02},
  abstract = {Due to the importance of underwater exploration in the development and utilization of deep-sea resources, underwater autonomous operation is more and more important to avoid the dangerous high-pressure deep-sea environment. For underwater autonomous operation, the intelligent computer vision is the most important technology. In an underwater environment, weak illumination and low-quality image enhancement, as a preprocessing procedure, is necessary for underwater vision. In this paper, a combination of max-RGB method and shades of gray method is applied to achieve the enhancement of underwater vision, and then a CNN (Convolutional Neutral Network) method for solving the weakly illuminated problem for underwater images is proposed to train the mapping relationship to obtain the illumination map. After the image processing, a deep CNN method is proposed to perform the underwater detection and classification, according to the characteristics of underwater vision, two improved schemes are applied to modify the deep CNN structure. In the first scheme, a convolution kernel is used on the feature map, and then a downsampling layer is added to resize the output to equal . In the second scheme, a downsampling layer is added firstly, and then the convolution layer is inserted in the network, the result is combined with the last output to achieve the detection. Through comparison with the Fast RCNN, Faster RCNN, and the original YOLO V3, scheme 2 is verified to be better in detecting underwater objects. The detection speed is about 50 FPS (Frames per Second), and mAP (mean Average Precision) is about 90\%. The program is applied in an underwater robot; the real-time detection results show that the detection and classification are accurate and fast enough to assist the robot to achieve underwater working operation.},
  langid = {english},
  file = {C:\Users\tommc\Zotero\storage\MTYJJLXW\Han et al. - 2020 - Underwater Image Processing and Object Detection B.pdf}
}

@online{heDeepResidualLearning2015a,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12-10},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1512.03385},
  url = {http://arxiv.org/abs/1512.03385},
  urldate = {2024-06-11},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tommc\\Zotero\\storage\\QWTERTIY\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;C\:\\Users\\tommc\\Zotero\\storage\\AH7VPI23\\1512.html}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016-06},
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  url = {https://ieeexplore.ieee.org/document/7780459},
  urldate = {2024-04-02},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Complexity theory,Degradation,Image recognition,Image segmentation,Neural networks,Training,Visualization},
  file = {C\:\\Users\\tommc\\Zotero\\storage\\IIGY7FJM\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf;C\:\\Users\\tommc\\Zotero\\storage\\T9QLZZFM\\7780459.html}
}

@inproceedings{jiangUnderwaterSpeciesDetection2021,
  title = {Underwater {{Species Detection}} Using {{Channel Sharpening Attention}}},
  author = {Jiang, Lihao and Wang, Yi and Jia, Qi and Xu, Shengwei and Liu, yu and Fan, Xin and Li, Haojie and Liu, Risheng and Xue, Xinwei and Wang, Ruili},
  date = {2021-10-17},
  pages = {4259--4267},
  doi = {10.1145/3474085.3475563},
  file = {C:\Users\tommc\Zotero\storage\KYLEUW2K\Jiang et al. - 2021 - Underwater Species Detection using Channel Sharpen.pdf}
}

@article{kamilarisDeepLearningAgriculture2018,
  title = {Deep Learning in Agriculture: {{A}} Survey},
  shorttitle = {Deep Learning in Agriculture},
  author = {Kamilaris, Andreas and Prenafeta-Boldú, Francesc X.},
  date = {2018-04-01},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Computers and Electronics in Agriculture},
  volume = {147},
  pages = {70--90},
  issn = {0168-1699},
  doi = {10.1016/j.compag.2018.02.016},
  url = {https://www.sciencedirect.com/science/article/pii/S0168169917308803},
  urldate = {2024-03-27},
  abstract = {Deep learning constitutes a recent, modern technique for image processing and data analysis, with promising results and large potential. As deep learning has been successfully applied in various domains, it has recently entered also the domain of agriculture. In this paper, we perform a survey of 40 research efforts that employ deep learning techniques, applied to various agricultural and food production challenges. We examine the particular agricultural problems under study, the specific models and frameworks employed, the sources, nature and pre-processing of data used, and the overall performance achieved according to the metrics used at each work under study. Moreover, we study comparisons of deep learning with other existing popular techniques, in respect to differences in classification or regression performance. Our findings indicate that deep learning provides high accuracy, outperforming existing commonly used image processing techniques.},
  keywords = {Agriculture,Convolutional Neural Networks,Deep learning,Food systems,Recurrent Neural Networks,Smart farming,Survey},
  file = {C\:\\Users\\tommc\\Zotero\\storage\\TNTGSGR9\\Kamilaris and Prenafeta-Boldú - 2018 - Deep learning in agriculture A survey.pdf;C\:\\Users\\tommc\\Zotero\\storage\\47YQM2YP\\S0168169917308803.html}
}

@article{liUnderwaterImageEnhancement2020,
  title = {An {{Underwater Image Enhancement Benchmark Dataset}} and {{Beyond}}},
  author = {Li, Chongyi and Guo, Chunle and Ren, Wenqi and Cong, Runmin and Hou, Junhui and Kwong, Sam and Tao, Dacheng},
  date = {2020},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {29},
  pages = {4376--4389},
  issn = {1941-0042},
  doi = {10.1109/TIP.2019.2955241},
  url = {https://ieeexplore.ieee.org/abstract/document/8917818?casa_token=pf7LS281xgAAAAAA:L9Ehqtej-odzNzvQw2Q-FU-zhw41vTkFOH-9l3auXZXUiRGpLG2SxRXfiIVsQPTT3MbqdZSXjZQ},
  urldate = {2024-04-02},
  abstract = {Underwater image enhancement has been attracting much attention due to its significance in marine engineering and aquatic robotics. Numerous underwater image enhancement algorithms have been proposed in the last few years. However, these algorithms are mainly evaluated using either synthetic datasets or few selected real-world images. It is thus unclear how these algorithms would perform on images acquired in the wild and how we could gauge the progress in the field. To bridge this gap, we present the first comprehensive perceptual study and analysis of underwater image enhancement using large-scale real-world images. In this paper, we construct an Underwater Image Enhancement Benchmark (UIEB) including 950 real-world underwater images, 890 of which have the corresponding reference images. We treat the rest 60 underwater images which cannot obtain satisfactory reference images as challenging data. Using this dataset, we conduct a comprehensive study of the state-of-the-art underwater image enhancement algorithms qualitatively and quantitatively. In addition, we propose an underwater image enhancement network (called Water-Net) trained on this benchmark as a baseline, which indicates the generalization of the proposed UIEB for training Convolutional Neural Networks (CNNs). The benchmark evaluations and the proposed Water-Net demonstrate the performance and limitations of state-of-the-art algorithms, which shed light on future research in underwater image enhancement. The dataset and code are available at https://li-chongyi.github.io/proj\_benchmark.html.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  keywords = {comprehensive evaluation,Convolutional neural networks,deep learning,Deep learning,Image enhancement,Image restoration,Object detection,real-world underwater images,Statistical analysis,Underwater image enhancement,Underwater technology},
  file = {C\:\\Users\\tommc\\Zotero\\storage\\42U8A4IJ\\Li et al. - 2020 - An Underwater Image Enhancement Benchmark Dataset .pdf;C\:\\Users\\tommc\\Zotero\\storage\\BFQXWCJV\\Li et al. - 2020 - An Underwater Image Enhancement Benchmark Dataset .pdf;C\:\\Users\\tommc\\Zotero\\storage\\YDR2HDTJ\\8917818.html}
}

@online{PapersCodeLSUI,
  title = {Papers with {{Code}} - {{LSUI Dataset}}},
  url = {https://paperswithcode.com/dataset/lsui},
  urldate = {2024-04-02},
  abstract = {We released a large-scale underwater image (LSUI) dataset including 5004 image pairs, which involve richer underwater scenes (lighting conditions, water types and target categories) and better visual quality reference images than the existing ones.},
  langid = {english},
  file = {C:\Users\tommc\Zotero\storage\I5GL56L9\lsui.html}
}

@online{pedersenBrackishMOTBrackishMultiObject2023,
  title = {{{BrackishMOT}}: {{The Brackish Multi-Object Tracking Dataset}}},
  shorttitle = {{{BrackishMOT}}},
  author = {Pedersen, Malte and Lehotský, Daniel and Nikolov, Ivan and Moeslund, Thomas B.},
  date = {2023-02-21},
  eprint = {2302.10645},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.10645},
  url = {http://arxiv.org/abs/2302.10645},
  urldate = {2024-06-11},
  abstract = {There exist no publicly available annotated underwater multi-object tracking (MOT) datasets captured in turbid environments. To remedy this we propose the BrackishMOT dataset with focus on tracking schools of small fish, which is a notoriously difficult MOT task. BrackishMOT consists of 98 sequences captured in the wild. Alongside the novel dataset, we present baseline results by training a state-of-the-art tracker. Additionally, we propose a framework for creating synthetic sequences in order to expand the dataset. The framework consists of animated fish models and realistic underwater environments. We analyse the effects of including synthetic data during training and show that a combination of real and synthetic underwater training data can enhance tracking performance. Links to code and data can be found at https://www.vap.aau.dk/brackishmot},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tommc\\Zotero\\storage\\V6LPU4YT\\Pedersen et al. - 2023 - BrackishMOT The Brackish Multi-Object Tracking Da.pdf;C\:\\Users\\tommc\\Zotero\\storage\\7EMPS2BQ\\2302.html}
}

@inproceedings{pedersenDetectionMarineAnimals2019,
  title = {Detection of {{Marine Animals}} in a {{New Underwater Dataset}} with {{Varying Visibility}}},
  author = {Pedersen, Malte and Bruslund Haurum, Joakim and Gade, Rikke and Moeslund, Thomas B.},
  date = {2019},
  pages = {18--26},
  url = {https://openaccess.thecvf.com/content_CVPRW_2019/html/AAMVEM/Pedersen_Detection_of_Marine_Animals_in_a_New_Underwater_Dataset_with_CVPRW_2019_paper.html},
  urldate = {2024-06-09},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  file = {C:\Users\tommc\Zotero\storage\EUHDTJ8U\Pedersen et al. - 2019 - Detection of Marine Animals in a New Underwater Da.pdf}
}

@article{pengUshapeTransformerUnderwater2023,
  title = {U-Shape {{Transformer}} for {{Underwater Image Enhancement}}},
  author = {Peng, Lintao and Zhu, Chunli and Bian, Liheng},
  date = {2023},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {32},
  eprint = {2111.11843},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  pages = {3066--3079},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2023.3276332},
  url = {http://arxiv.org/abs/2111.11843},
  urldate = {2024-04-02},
  abstract = {The light absorption and scattering of underwater impurities lead to poor underwater imaging quality. The existing data-driven based underwater image enhancement (UIE) techniques suffer from the lack of a large-scale dataset containing various underwater scenes and high-fidelity reference images. Besides, the inconsistent attenuation in different color channels and space areas is not fully considered for boosted enhancement. In this work, we constructed a large-scale underwater image (LSUI) dataset including 5004 image pairs, and reported an U-shape Transformer network where the transformer model is for the first time introduced to the UIE task. The U-shape Transformer is integrated with a channel-wise multi-scale feature fusion transformer (CMSFFT) module and a spatial-wise global feature modeling transformer (SGFMT) module, which reinforce the network's attention to the color channels and space areas with more serious attenuation. Meanwhile, in order to further improve the contrast and saturation, a novel loss function combining RGB, LAB and LCH color spaces is designed following the human vision principle. The extensive experiments on available datasets validate the state-of-the-art performance of the reported technique with more than 2dB superiority.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\tommc\\Zotero\\storage\\38SFX423\\Peng et al. - 2023 - U-shape Transformer for Underwater Image Enhanceme.pdf;C\:\\Users\\tommc\\Zotero\\storage\\9SIFNQL7\\2111.html}
}

@inproceedings{perezDeepLearningApproach2017,
  title = {A {{Deep Learning Approach}} for {{Underwater Image Enhancement}}},
  booktitle = {Biomedical {{Applications Based}} on {{Natural}} and {{Artificial Computing}}},
  author = {Perez, Javier and Attanasio, Aleks C. and Nechyporenko, Nataliya and Sanz, Pedro J.},
  editor = {Ferrández Vicente, José Manuel and Álvarez-Sánchez, José Ramón and family=Paz López, given=Félix, prefix=de la, useprefix=true and Toledo Moreo, Javier and Adeli, Hojjat},
  date = {2017},
  pages = {183--192},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-59773-7_19},
  abstract = {Image processing in underwater robotics is one of the most challenging problems in autonomous underwater robotics due to light transmission in water. Although image restoration techniques are able to correctly remove the haze in a degraded image they need many images from the same location making impossible to use it in a real time system. Taking into account the great results of deep learning techniques in other image processing problems such as colorizing images or detecting objects a deep learning solution is proposed. A convolutional neural network is trained with image restoration techniques to dehaze single images outperforming other image enhancement techniques. The proposed approach is able to produce image restoration quality images with a single image as input. The neural network is validated using images from different locations and characteristics to prove the generalization capabilities.},
  isbn = {978-3-319-59773-7},
  langid = {english},
  keywords = {Deep learning,Image dehazing,Underwater robotics},
  file = {C:\Users\tommc\Zotero\storage\D8NXWXZX\Perez et al. - 2017 - A Deep Learning Approach for Underwater Image Enha.pdf}
}

@online{renFasterRCNNRealTime2016,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R-CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  date = {2016-01-06},
  eprint = {1506.01497},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1506.01497},
  url = {http://arxiv.org/abs/1506.01497},
  urldate = {2024-06-12},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tommc\\Zotero\\storage\\NXYSGMA7\\Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection w.pdf;C\:\\Users\\tommc\\Zotero\\storage\\F737T6VB\\1506.html}
}

@article{rizziniInvestigationVisionBasedUnderwater2015,
  title = {Investigation of {{Vision-Based Underwater Object Detection}} with {{Multiple Datasets}}},
  author = {Rizzini, Dario Lodi and Kallasi, Fabjan and Oleari, Fabio and Caselli, Stefano},
  date = {2015-06-01},
  journaltitle = {International Journal of Advanced Robotic Systems},
  volume = {12},
  number = {6},
  pages = {77},
  publisher = {SAGE Publications},
  issn = {1729-8806},
  doi = {10.5772/60526},
  url = {https://doi.org/10.5772/60526},
  urldate = {2024-03-27},
  abstract = {In this paper, we investigate the potential of vision-based object detection algorithms in underwater environments using several datasets to highlight the issues arising in different scenarios. Underwater computer vision has to cope with distortion and attenuation due to light propagation in water, and with challenging operating conditions. Scene segmentation and shape recognition in a single image must be carefully designed to achieve robust object detection and to facilitate object pose estimation. We describe a novel multi-feature object detection algorithm conceived to find human-made artefacts lying on the seabed. The proposed method searches for a target object according to a few general criteria that are robust to the underwater context, such as salient colour uniformity and sharp contours. We assess the performance of the proposed algorithm across different underwater datasets. The datasets have been obtained using stereo cameras of different quality, and diverge for the target object type and colour, acquisition depth and conditions. The effectiveness of the proposed approach has been experimentally demonstrated. Finally, object detection is discussed in connection with the simple colour-based segmentation and with the difficulty of tri-dimensional processing on noisy data.},
  langid = {english},
  file = {C:\Users\tommc\Zotero\storage\P99A8EVQ\Rizzini et al. - 2015 - Investigation of Vision-Based Underwater Object De.pdf}
}

@inproceedings{rosliUnderwaterAnimalDetection2021,
  title = {Underwater {{Animal Detection Using YOLOV4}}},
  booktitle = {2021 11th {{IEEE International Conference}} on {{Control System}}, {{Computing}} and {{Engineering}} ({{ICCSCE}})},
  author = {Rosli, Mohamed Syazwan Asyraf Bin and Isa, Iza Sazanita and Maruzuki, Mohd Ikmal Fitri and Sulaiman, Siti Noraini and Ahmad, Ibrahim},
  date = {2021-08},
  pages = {158--163},
  doi = {10.1109/ICCSCE52189.2021.9530877},
  url = {https://ieeexplore-ieee-org.ezproxy.aut.ac.nz/document/9530877},
  urldate = {2024-06-09},
  abstract = {Underwater computer vision system has been widely used for many underwater applications such as ocean exploration, biological research and monitoring underwater life sustainability. However, in counterpart of the underwater environment, there are several challenges arise such as water murkiness, dynamic background, low light and low visibility which limits the ability to explore this area. To overcome these challenges, there is a crucial to improve underwater vision system that able to efficiently adapt with varying environments. Therefore, it is great of significance to propose an efficient and precise underwater detection by using YOLOv4 based on deep learning algorithm. In the research, an open-source underwater dataset was used to investigate YOLOv4 performance based on metrics evaluation of precision and processing speed (FPS). The result shows that YOLOv4 able to achieve a remarkable of 97.96\% for mean average precision with frame per second of 46.6. This study shows that YOLOv4 model is highly significant to be implemented in underwater vision system as it possesses ability to accurately detect underwater objects with haze and low-light environments.},
  eventtitle = {2021 11th {{IEEE International Conference}} on {{Control System}}, {{Computing}} and {{Engineering}} ({{ICCSCE}})},
  keywords = {Biology,computer vision,Machine vision,mean average precision,Oceans,real-time,Real-time systems,Robustness,Sustainable development,Underwater detection,Unmanned vehicles,YOLOv4},
  file = {C\:\\Users\\tommc\\Zotero\\storage\\VR8X8H4N\\Rosli et al. - 2021 - Underwater Animal Detection Using YOLOV4.pdf;C\:\\Users\\tommc\\Zotero\\storage\\XK2AXBEC\\9530877.html}
}

@article{shafiqDeepResidualLearning2022,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}: {{A Survey}}},
  shorttitle = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {Shafiq, Muhammad and Gu, Zhaoquan},
  date = {2022-01},
  journaltitle = {Applied Sciences},
  volume = {12},
  number = {18},
  pages = {8972},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app12188972},
  url = {https://www.mdpi.com/2076-3417/12/18/8972},
  urldate = {2024-04-02},
  abstract = {Deep Residual Networks have recently been shown to significantly improve the performance of neural networks trained on ImageNet, with results beating all previous methods on this dataset by large margins in the image classification task. However, the meaning of these impressive numbers and their implications for future research are not fully understood yet. In this survey, we will try to explain what Deep Residual Networks are, how they achieve their excellent results, and why their successful implementation in practice represents a significant advance over existing techniques. We also discuss some open questions related to residual learning as well as possible applications of Deep Residual Networks beyond ImageNet. Finally, we discuss some issues that still need to be resolved before deep residual learning can be applied on more complex problems.},
  issue = {18},
  langid = {english},
  keywords = {deep residual learning,deep residual learning for image recognition,image processing,image recognition},
  file = {C:\Users\tommc\Zotero\storage\WIXC5CGB\Shafiq and Gu - 2022 - Deep Residual Learning for Image Recognition A Su.pdf}
}

@online{szegedyGoingDeeperConvolutions2014,
  title = {Going {{Deeper}} with {{Convolutions}}},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  date = {2014-09-16},
  eprint = {1409.4842},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1409.4842},
  url = {http://arxiv.org/abs/1409.4842},
  urldate = {2024-03-27},
  abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tommc\\Zotero\\storage\\MQ8EYNSN\\Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf;C\:\\Users\\tommc\\Zotero\\storage\\64XRYAI7\\1409.html}
}

@article{tengUnderwaterTargetRecognition2020,
  title = {Underwater Target Recognition Methods Based on the Framework of Deep Learning: {{A}} Survey},
  shorttitle = {Underwater Target Recognition Methods Based on the Framework of Deep Learning},
  author = {Teng, Bowen and Zhao, Hongjian},
  date = {2020-11-01},
  journaltitle = {International Journal of Advanced Robotic Systems},
  volume = {17},
  number = {6},
  pages = {1729881420976307},
  publisher = {SAGE Publications},
  issn = {1729-8806},
  doi = {10.1177/1729881420976307},
  url = {https://doi.org/10.1177/1729881420976307},
  urldate = {2024-04-02},
  abstract = {The accuracy of underwater target recognition by autonomous underwater vehicle (AUV) is a powerful guarantee for underwater detection, rescue, and security. Recently, deep learning has made significant improvements in digital image processing for target recognition and classification, which makes the underwater target recognition study becoming a hot research field. This article systematically describes the application of deep learning in underwater image analysis in the past few years and briefly expounds the basic principles of various underwater target recognition methods. Meanwhile, the applicable conditions, pros and cons of various methods are pointed out. The technical problems of AUV underwater dangerous target recognition methods are analyzed, and corresponding solutions are given. At the same time, we prospect the future development trend of AUV underwater target recognition.},
  langid = {english},
  file = {C:\Users\tommc\Zotero\storage\S3AW5L7F\Teng and Zhao - 2020 - Underwater target recognition methods based on the.pdf}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2024-06-10},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\tommc\\Zotero\\storage\\96SIGUV2\\Vaswani et al. - 2023 - Attention Is All You Need.pdf;C\:\\Users\\tommc\\Zotero\\storage\\FP97E2LE\\1706.html}
}

@article{xu2023,
  title = {A Systematic Review and Analysis of Deep Learning-Based Underwater Object Detection},
  author = {Xu, Shubo and Zhang, Minghua and Song, Wei and Mei, Haibin and He, Qi and Liotta, Antonio},
  date = {2023-03-28},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {527},
  pages = {204--232},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2023.01.056},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231223000656},
  urldate = {2024-03-26},
  abstract = {Underwater object detection is one of the most challenging research topics in computer vision technology. The complex underwater environment makes underwater images suffer from high noise, low visibility, blurred edges, low contrast and color deviation, which brings significant challenges to underwater object detection tasks. In underwater object detection tasks, traditional object detection methods often perform poorly in terms of accuracy and generalization capabilities. Underwater object detection requires accurate, stable, generalizable, real-time and lightweight detection models, for which many researchers have proposed various underwater object detection techniques based on deep learning. Although many outstanding results have been achieved on underwater object detection over the years, the research status of underwater object detection techniques are still lack of unified induction, and some existing problems need to be further probed from the latest perspective. In addition, previous reviews lack analysis on the relationship between underwater image enhancement and object detection. Therefore, this paper provides a comprehensive review of the current research challenges, future development trends, and potential applications of underwater object detection techniques. More importantly, this paper has explored the internal relationship between underwater image enhancement and object detection, and analyzed the possible implementation manners of underwater image enhancement in the object detection task in order to further enhance its benefits. The experiments show the performances of current underwater image enhancement and state-of-the-art object detection algorithms, point out their limitations, and indicate that there is not a strict positive correlation between underwater image enhancement and the accuracy improvement of object detection. The domain shift caused by underwater image enhancement cannot be ignored. This paper can be regarded as a guide for future works on underwater object detection.},
  keywords = {Computer vision,Convolutional neural networks,Deep learning,Underwater image enhancement,Underwater object detection},
  file = {C:\Users\tommc\Zotero\storage\99AC937X\S0925231223000656.html}
}

@inproceedings{zhiqiangReviewObjectDetection2017,
  title = {A Review of Object Detection Based on Convolutional Neural Network},
  booktitle = {2017 36th {{Chinese Control Conference}} ({{CCC}})},
  author = {Zhiqiang, Wang and Jun, Liu},
  date = {2017-07},
  pages = {11104--11109},
  issn = {1934-1768},
  doi = {10.23919/ChiCC.2017.8029130},
  url = {https://ieeexplore.ieee.org/abstract/document/8029130?casa_token=b7SfIX-apSIAAAAA:xLgz_MEE9A0FqqhzvHDhgGuTJL_3hfKcnYlmtaMn9-jjkd_y1yl6N0PnjlNEEG0UKvUGe0feBo4},
  urldate = {2024-04-02},
  abstract = {With the development of intelligent device and social media, the data bulk on Internet has grown with high speed. As an important aspect of image processing, object detection has become one of the international popular research fields. In recent years, the powerful ability with feature learning and transfer learning of Convolutional Neural Network (CNN) has received growing interest within the computer vision community, thus making a series of important breakthroughs in object detection. So it is a significant survey that how to apply CNN to object detection for better performance. First the paper introduced the basic concept and architecture of CNN. Secondly the methods that how to solve the existing problems of conventional object detection are surveyed, mainly analyzing the detection algorithm based on region proposal and based on regression. Thirdly it mentioned some means which improve the performance of object detection. Then the paper introduced some public datasets of object detection and the concept of evaluation criterion. Finally, it combed the current research achievements and thoughts of object detection, summarizing the important progress and discussing the future directions.},
  eventtitle = {2017 36th {{Chinese Control Conference}} ({{CCC}})},
  keywords = {Computer vision,Convolutional Neural Network,datasets,Feature extraction,object detection,Object detection,Proposals,region proposal,regression,Training},
  file = {C\:\\Users\\tommc\\Zotero\\storage\\WME75GMU\\Zhiqiang and Jun - 2017 - A review of object detection based on convolutiona.pdf;C\:\\Users\\tommc\\Zotero\\storage\\5JGCPR2B\\8029130.html}
}
